---
title: "Class Activity, January 24"
output: 
  tufte::tufte_html:
    css: "lab.css"
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

In this class activity, you will practice maximum likelihood estimation and logistic regression. In Part I, you will practice using calculus to compute the maximum likelihood estimate (MLE). In Part II, you will fit a logistic regression model in R.

# Part I

Suppose we have a variable $Y$ that can take **three** possible outcomes: $Y = -1, 0$, or $1$. We also know that

* $P(Y_i = 0) = \pi_0$
* $P(Y_i = -1) = 2 \pi_0$
* $P(Y_i = 1) = 1 - 3 \pi_0$

where the parameter $\pi_0$ is unknown. We observe data $-1, -1, 0, 1, 0, -1$, and we want to estimate $\pi_0$ from the data.

From last class, we know that 

$\log L(\widehat{\pi}_0) = 3 \log(2) + 3 \log(\widehat{\pi}_0) + 2 \log( \widehat{\pi}_0) + \log(1 - 3 \widehat{\pi}_0)$

1. Use calculus to calculate the maximum likelihood estimate $\widehat{\pi}_0$. Remember to differentiate the log likelihood, and recall the following rules for derivatives:

* $\dfrac{d}{dx} \log x = \dfrac{1}{x}$
* $\dfrac{d}{dx} c f(x) = c \dfrac{d}{dx} f(x)$ for constant $c$
* $\dfrac{d}{dx} (f(x) + c) = \dfrac{d}{dx} f(x)$ for constant $c$
* $\dfrac{d}{dx} (f(x) + g(x)) = \dfrac{d}{dx} f(x) + \dfrac{d}{dx} g(x)$


# Part II

In the slides, we fit a logistic regression model in R on the grad application data from Lab 1, with GPA as a predictor. Now let's consider GRE as a predictor instead. Our logistic regression model is

$Y_i \sim Bernoulli(\pi_i) \hspace{0.5cm} \log \left( \dfrac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 \text{GRE_i}$

2. Fit this logistic regression model in R, and report the equation of the fitted model.

3. What is the deviance of the model you fit in Exercise 2? What is the log likelihood?

4. The deviance of the logistic regression model with GPA as the predictor was 487. Compare this to your deviance from Exercise 3. Which variable (GPA or GRE score) gives a better fit to the data? (For now we are only considering models with a single predictor).