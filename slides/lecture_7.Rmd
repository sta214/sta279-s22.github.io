---
title: Fitting logistic regression
output:
  xaringan::moon_reader:
    css: "lab-slides.css"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

### Agenda

.large[
* Recap from last class
* Continuing maximum likelihood estimation
* Fitting logistic regression in R
* Reminder: HW 1 due Friday at noon on Canvas
]

---

### Recap

.large[
**Definition:** The *likelihood* $L(Model) = P(Data | Model)$ of a model is the probability of the observed data, given that we assume a certain model and certain values for the parameters that define that model.
]

.large[
Coin example: flip a coin 5 times, with $\pi_i = P(Heads)$
* Model: $Y_i \sim Bernoulli(\pi_i)$, and $\widehat{\pi}_i = 0.9$
* Data: $y_1,...,y_5 = T, T, T, T, H$
* Likelihood: $L(\widehat{\pi}_i) = P(y_1,...y_5 | \pi_i = 0.9) = 0.00009$
]

---

### Recap

.large[
**Maximum likelihood estimation:** pick the parameter estimate that maximizes the likelihood.

Coin example: flip a coin 5 times, with $\pi_i = P(Heads)$

* Observed data: T, T, T, T, H
* Likelihood: $L(\widehat{\pi}_i) = (1 - \widehat{\pi}_i)^4(\widehat{\pi}_i)$
* Choose $\widehat{\pi}_i$ to maximize $L(\widehat{\pi}_i)$
]

---

### Recap

.large[
Some methods for maximum likelihood estimation:

* Calculate $L(\widehat{\pi}_i)$ for different values of $\widehat{\pi}_i$ in R. Pick $\widehat{\pi}_i$ that gives highest likelihood.
  * Downside: can't check every $\widehat{\pi}_i$
* Use calculus: 
  * Take the log likelihood, $\log L(\widehat{\pi}_i)$
  * Differentiate and set equal to 0
  * Solve for $\widehat{\pi}_i$
]


---

### Recap

.large[
Log likelihood:
.center[
$\log L(\widehat{\pi}_i) = k \log (\widehat{\pi}_i) + (n - k) \log (1 - \widehat{\pi}_i)$
]
]

.large[
Take the first derivative, and set equal to 0:

\begin{align}
\dfrac{d}{d \widehat{\pi}_i} \log L(\widehat{\pi}_i) &= \dfrac{k}{\widehat{\pi}_i} - \dfrac{n - k}{1 - \widehat{\pi}_i} \\
&= 0
\end{align}
]

---

### Recap

.large[
Log likelihood:
.center[
$\log L(\widehat{\pi}_i) = k \log (\widehat{\pi}_i) + (n - k) \log (1 - \widehat{\pi}_i)$
]
]

.large[

\begin{align}
\dfrac{d}{d \widehat{\pi}_i} \log L(\widehat{\pi}_i) &= \dfrac{k}{\widehat{\pi}_i} - \dfrac{n - k}{1 - \widehat{\pi}_i} \\
&= 0
\end{align}
]

.large[
.center[
$\dfrac{k}{\widehat{\pi}_i} = \dfrac{n - k}{1 - \widehat{\pi}_i}$

$k(1 - \widehat{\pi}_i) = (n-k)\widehat{\pi}_i$

$k - k \widehat{\pi}_i = n \widehat{\pi}_i - k \widehat{\pi}_i$

$\widehat{\pi}_i = \dfrac{k}{n}$
]
]

---

### Class activity, Part I

.large[
[https://sta279-s22.github.io/class_activities/ca_lecture_7.html](https://sta279-s22.github.io/class_activities/ca_lecture_7.html)
]

---

### Class activity

.large[
.center[
$\log L(\widehat{\pi}_0) = 3 \log(2) + 3 \log(\widehat{\pi}_0) + 2 \log( \widehat{\pi}_0) + \log(1 - 3 \widehat{\pi}_0)$

$\dfrac{d}{d \widehat{\pi}_0} \log L(\widehat{\pi}_0) = \ ?$
]
]

---

### Class activity

.large[
.center[
$\log L(\widehat{\pi}_0) = 3 \log(2) + 3 \log(\widehat{\pi}_0) + 2 \log( \widehat{\pi}_0) + \log(1 - 3 \widehat{\pi}_0)$

$\dfrac{d}{d \widehat{\pi}_0} \log L(\widehat{\pi}_0) = \dfrac{3}{\widehat{\pi}_0} + \dfrac{2}{\widehat{\pi}_0} - \dfrac{3}{1 - 3 \widehat{\pi}_0} = 0$
]
]

--

.large[
.center[
$\dfrac{5}{\widehat{\pi}_0} = \dfrac{3}{1 - 3\widehat{\pi}_0}$

$\dfrac{1 - 3\widehat{\pi}_0}{\widehat{\pi}_0} = \dfrac{3}{5}$

$\dfrac{1}{\widehat{\pi}_0} = \dfrac{18}{5}$

$\widehat{\pi}_0 = \dfrac{5}{18} = 0.278$
]
]

---

### Connecting it back to logistic regression

.large[
* If $Y_i \sim Bernoulli(\pi_i)$, and $\pi_i$ doesn't depend on any explanatory variables, we can find the maximum likelihood estimate (MLE) $\widehat{\pi}_i$
* In the class activity, we found the MLE $\widehat{\pi}_0$ for a different distribution too
* How does this connect to logistic regression?
]

---

### Logistic regression

.large[
.center[
$Y_i \sim Bernoulli(\pi_i) \hspace{0.5cm} \log \left( \dfrac{\pi_i}{1 - \pi_i} \right) = \beta_0 + \beta_1 X_i$
]
]

.large[
Or in other words, 
.center[
$\pi_i = \dfrac{\exp\{\beta_0 + \beta_1 X_i\}}{1 + \exp\{\beta_0 + \beta_1 X_i\}}$
]
]

--

.large[
* To fit this model, we need to obtain estimates $\widehat{\beta}_0$ and $\widehat{\beta}_1$
* Let's start by exploring the likelihood with this model
]

---

### Logistic regression likelihood

.large[
.center[
$Y_i \sim Bernoulli(\pi_i) \hspace{0.5cm} \pi_i = \dfrac{\exp\{\beta_0 + \beta_1 X_i\}}{1 + \exp\{\beta_0 + \beta_1 X_i\}}$
]
]

.large[
Data: $(X_1, Y_1),...,(X_n, Y_n)$

Likelihood:
.center[
$L(\beta_0, \beta_1) = \prod \limits_{i=1}^n \pi_i^{Y_i}(1 - \pi_i)^{1 - Y_i}$

$\log L(\beta_0, \beta_1) = \sum \limits_{i=1}^n \left( Y_i \log(\pi_i) + (1 - Y_i) \log(1 - \pi_i) \right)$
]
]

---

### Logistic regression log likelihood

.large[
\begin{align}
\log L(\beta_0, \beta_1) &= \sum \limits_{i=1}^n \left( Y_i \log(\pi_i) + (1 - Y_i) \log(1 - \pi_i) \right) \\
&= \sum \limits_{i=1}^n Y_i \log(\pi_i) + \sum \limits_{i=1}^n (1 - Y_i) \log(1 - \pi_i) \\
&= \sum \limits_{i=1}^n Y_i \log \left( \dfrac{e^{\beta_0 + \beta_1 X_i}}{1 + e^{\beta_0 + \beta_1 X_i}} \right) + \\
& \hspace{1cm} \sum \limits_{i=1}^n (1-Y_i) \log \left(1 - \dfrac{e^{\beta_0 + \beta_1 X_i}}{1 + e^{\beta_0 + \beta_1 X_i}} \right)
\end{align}
]

---

### Logistic regression log likelihood

.large[
\begin{align}
\log L(\beta_0, \beta_1) &= \sum \limits_{i=1}^n Y_i \log \left( \dfrac{e^{\beta_0 + \beta_1 X_i}}{1 + e^{\beta_0 + \beta_1 X_i}} \right) + \\
& \hspace{1cm} \sum \limits_{i=1}^n (1-Y_i) \log \left(1 - \dfrac{e^{\beta_0 + \beta_1 X_i}}{1 + e^{\beta_0 + \beta_1 X_i}} \right)
\end{align}
]

.large[
* Because we have two parameters, $\beta_0$ and $\beta_1$, the situation is more difficult
* The math to find MLEs $\widehat{\beta}_0$ and $\widehat{\beta}_1$ is more complex than we will cover
* R calculates $\widehat{\beta}_0$ and $\widehat{\beta}_1$ for us
]

---

### Logistic regression in R

.large[
**Data:** Grad application data
  * `admit`: accepted to grad school? (0 = no, 1 = yes)
  * `gre`: GRE score
  * `gpa`: undergrad GPA
  * `rank`: prestige of undergrad institution
  
Let's fit a logistic regression model with GPA as the predictor.
]

--

.large[
```r
glm(admit ~ gpa, family = binomial, data = grad_app)
```
]

---

### Logistic regression in R

.large[
```r
glm(admit ~ gpa, family = binomial, data = grad_app)
```

* `admit` is the response
* `gpa` is the predictor
* `family = binomial` means "fit logistic regression"
* Use `glm` (instead of `lm`) to fit regression models other than linear regression (e.g., logistic regression, Poisson regression, etc.)
]

---

### Logistic regression in R

```{r, include=F}
grad_app <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
```

.large[
```{r}
glm(admit ~ gpa, family = binomial, data = grad_app)
```

.question[
What are $\widehat{\beta}_0$ and $\widehat{\beta}_1$?
]
]

---

### Logistic regression in R

```{r, include=F}
grad_app <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
```

.large[
```{r}
glm(admit ~ gpa, family = binomial, data = grad_app)
```

$\log \left( \dfrac{\widehat{\pi}_i}{1 - \widehat{\pi}_i} \right) = -4.358 + 1.051 \ \text{GPA}_i$
]

---

### Logistic regression in R

```{r, include=F}
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
   lines <- options$output.lines
   if (is.null(lines)) {
     return(hook_output(x, options))  # pass to default hook
   }
   x <- unlist(strsplit(x, "\n"))
   more <- "..."
   if (length(lines)==1) {        # first n lines
     if (length(x) > lines) {
       # truncate the output, but add ....
       x <- c(head(x, lines), more)
     }
   } else {
     x <- c(more, x[lines], more)
   }
   # paste these lines together
   x <- paste(c(x, ""), collapse = "\n")
   hook_output(x, options)
 })
```

.large[
```{r, output.lines = 8:10}
glm(admit ~ gpa, family = binomial, data = grad_app)
```

* For linear regression, the bottom part of the output usually contains things like $R^2$ and $R^2_{adj}$ -- measures of how well the model fits the data.

.question[
What quantity have we been exploring that allows us to evaluate how well the model fits the data?
]
]

---

### Logistic regression in R

.large[
```{r, echo=F, output.lines = 8:10}
glm(admit ~ gpa, family = binomial, data = grad_app)
```

* For linear regression, the bottom part of the output usually contains things like $R^2$ and $R^2_{adj}$ -- measures of how well the model fits the data.

.question[
What quantity have we been exploring that allows us to evaluate how well the model fits the data?
]

The likelihood!
]

---

### Logistic regression in R

.large[
```{r, echo=F, output.lines = 8:10}
glm(admit ~ gpa, family = binomial, data = grad_app)
```

* For linear regression, the bottom part of the output usually contains things like $R^2$ and $R^2_{adj}$ -- measures of how well the model fits the data.

.question[
Does R report the likelihood of the fitted model?
]
]

--

.large[
Not quite.
]

---

### Deviance

.large[
R reports the *deviance*, rather than the likelihood.

**Deviance:** Let $L$ be the likelihood. The *deviance* is defined as $-2 \log L$.

* Numbers are more manageable
* For *linear* regression, deviance is the same as SSE
* Larger likelihood $\rightarrow$ smaller deviance
]

---

### Deviance

.large[
```{r, echo=F, output.lines = 8:10}
glm(admit ~ gpa, family = binomial, data = grad_app)
```

Deviance $= -2 \log L = 487$
]

---

### Class activity, Part II

.large[
[https://sta279-s22.github.io/class_activities/ca_lecture_7.html](https://sta279-s22.github.io/class_activities/ca_lecture_7.html)
]

---

### Class activity: fitted model

.large[
```{r}
glm(admit ~ gre, family = binomial, data = grad_app)
```
]

--

.large[
$\log \left( \dfrac{\widehat{\pi}_i}{1 - \widehat{\pi}_i} \right) = -2.9013 + 0.00358 \ \text{GRE}_i$
]

---

### Class activity: deviance

.large[
```{r, output.lines=8:10}
glm(admit ~ gre, family = binomial, data = grad_app)
```
]

--

.large[
Deviance = 486.1

Log likelihood = $-0.5 \cdot 486.1 = -243.05$
]

---

### Class activity: deviance

.large[
```{r, output.lines=8:10}
glm(admit ~ gre, family = binomial, data = grad_app)
```
]

.large[
.question[
Which predictor (GRE or GPA) gives a model with a better fit?
]
]

--

.large[
GRE has a slightly smaller deviance (486.1 vs. 487), so GRE gives a slightly better fit.
]