---
title: Maximum likelihood estimation
output:
  xaringan::moon_reader:
    css: "lab-slides.css"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

### Likelihood

.large[
**Definition:** The *likelihood* $L(Model) = P(Data | Model)$ of a model is the probability of the observed data, given that we assume a certain model and certain values for the parameters that define that model.
]

--

.large[
Coin example: flip a coin 5 times, with $\pi_i = P(Heads)$
* Model: $Y_i \sim Bernoulli(\pi_i)$, and $\widehat{\pi}_i = 0.9$
* Data: $y_1,...,y_5 = T, T, T, T, H$
* Likelihood: $L(\widehat{\pi}_i) = P(y_1,...y_5 | \pi_i = 0.9) = 0.00009$
]

---

### Likelihood

.large[
Coin example: flip a coin 5 times, with $\pi_i = P(Heads)$. 

Observed data: T, T, T, T, H

Compare $\widehat{\pi}_i = 0.9$ and $\widehat{\pi}_i = 0.1$

* $L(0.9) = (0.1)(0.1)(0.1)(0.1)(0.9) = (0.1)^4(0.9) = 0.00009$
* $L(0.1) = (0.9)(0.9)(0.9)(0.9)(0.1) = (0.9)^4(0.1) = 0.0656$

.question[
Which estimate, 0.1 or 0.9, is a better estimate?
]
]

--

.large[
0.1
]

---

### Maximum likelihood

.large[
.question[
$\widehat{\pi}_i = 0.1$ is a better estimate than $\widehat{\pi}_i = 0.9$. But can we do any better? Is there a different value of $\widehat{\pi}_i$ with a higher likelihood?
]
]

--

.large[
**Maximum likelihood estimation:** Choose value of $\widehat{\pi}_i$ that maximizes the likelihood 
* **Option 1:** Compute likelihood for other values of $\widehat{\pi}_i$ by hand (tedious)
* **Option 2:** Compute likelihood for other values of $\widehat{\pi}_i$ in R
* **Option 3:** Maximize likelihood using calculus
]

---

### Computing likelihood in R

.large[
Observed data: T, T, T, T, H

* We are going to consider several different potential values for $\widehat{\pi}_i$:

.center[
$0, 0.1, 0.2, 0.3,..., 0.9, 1$
]

* For each potential value, we will compute the likelihood:

.center[
$L(\widehat{\pi}_i) = (1 - \widehat{\pi}_i)^4(\widehat{\pi}_i)$
]

* We then see which value has the highest likelihood.
* Is this all possible values? No, but let's start here.
]

---

### R code

.large[
```r
# List the values for pi hat
pi_hat <- seq(from = 0, to = 1, by = 0.1)
```
]
--

.large[
* This creates a sequence (`seq`) from 0 to 1, in increments of 0.1

.center[
$0, 0.1, 0.2, 0.3,..., 0.9, 1$
]
]

---

### R code

.large[
```r
# List the values for pi hat
pi_hat <- seq(from = 0, to = 1, by = 0.1)

# Create a space to store the likelihoods
likelihood <- rep(0,length(pi_hat))
```
]
--

.large[
* We then create a vector filled with 11 zeros
* Think about this as a storage box
* We will replace the 0s with the likelihoods as we calculate them
]

---

### R code

.large[
```r
# List the values for pi hat
pi_hat <- seq(from = 0, to = 1, by = 0.1)

# Create a space to store the likelihoods
likelihood <- rep(0,length(pi_hat))

# Compute and store the likelihoods
for( i in 1:length(pi_hat) ){
  likelihood[i] <- pi_hat[i]*(1-pi_hat[i])^4
}
```
]

--

.large[
* This last piece of code is called a *for loop*
* We compute the likelihood for each value of $\widehat{\pi}_i$ we are considering
]


---

### Results

```{r, echo=F}
# List the values for pi hat
pi_hat <- seq(from = 0, to = 1, by = 0.1)

# Create a space to store the likelihoods
likelihood <- rep(0,length(pi_hat))

# Compute and store the likelihoods
for( i in 1:length(pi_hat) ){
  likelihood[i] <- pi_hat[i]*(1-pi_hat[i])^4
}

knitr::kable(data.frame(pi_hat = pi_hat, likelihood = likelihood))
  
```

.large[
.question[
Which estimate $\widehat{\pi}_i$ gives us the largest likelihood?
]
]

---

### Maximum likelihood estimation

.large[
* Our estimate so far is $\widehat{\pi}_i = 0.2$. But we haven't explored all possible values
* Let's try some more!

.center[
$0, 0.01, 0.02, ..., 0.98, 0.99, 1$
]
]

---

### R code

.large[
.question[
Which code do I need to change for our new values?
]
]

.large[
```r
# List the values for pi hat
pi_hat <- seq(from = 0, to = 1, by = 0.1)

# Create a space to store the likelihoods
likelihood <- rep(0,length(pi_hat))

# Compute and store the likelihoods
for( i in 1:length(pi_hat) ){
  likelihood[i] <- pi_hat[i]*(1-pi_hat[i])^4
}
```
]

---

### R code

.large[
Just the first part (creating `pi_hat`)

```r
# List the values for pi hat
pi_hat <- seq(from = 0, to = 1, by = 0.01)

# Create a space to store the likelihoods
likelihood <- rep(0,length(pi_hat))

# Compute and store the likelihoods
for( i in 1:length(pi_hat) ){
  likelihood[i] <- pi_hat[i]*(1-pi_hat[i])^4
}
```
]

---

### Results

```{r, echo=F, message=F, warning=F, fig.width=8, fig.height=6, fig.align='center'}
library(tidyverse)

# List the values for pi hat
pi_hat <- seq(from = 0, to = 1, by = 0.01)

# Create a space to store the likelihoods
likelihood <- rep(0,length(pi_hat))

# Compute and store the likelihoods
for( i in 1:length(pi_hat) ){
  likelihood[i] <- pi_hat[i]*(1-pi_hat[i])^4
}

data.frame(pi_hat = pi_hat, likelihood = likelihood) %>%
  ggplot(aes(x = pi_hat, y = likelihood)) +
  geom_point(size=2) +
  theme_bw() +
  theme(text = element_text(size = 20))
  
```

.large[
The largest likelihood is again at $\widehat{\pi}_i = 0.2$
]

---

### Class Activity, Part I

.large[
[https://sta279-s22.github.io/class_activities/ca_lecture_6.html](https://sta279-s22.github.io/class_activities/ca_lecture_6.html)
]

---

### Class Activity

.large[
.center[
$L(\widehat{\pi}_0) = \ ?$
]
]

---

### Class Activity

.large[
.center[
$L(\widehat{\pi}_0) = (2 \widehat{\pi}_0)^3 (\widehat{\pi}_0)^2(1 - 3 \widehat{\pi}_0)$

$\log L(\widehat{\pi}_0) = 3 \log(2) + 3 \log(\widehat{\pi}_0) + 2 \log( \widehat{\pi}_0) + \log(1 - 3 \widehat{\pi}_0)$
]
]

---

### Class Activity

.large[
```r
# List the values for pi hat
pi_hat <- seq(from = 0, to = 1, by = 0.05)

# Create a space to store the likelihoods
likelihood <- rep(0,length(pi_hat))

# Compute and store the likelihoods
for( i in 1:length(pi_hat) ){
  likelihood[i] <- (2*pi_hat[i])^3 * 
      (pi_hat[i])^2 * (1 - 3 * pi_hat[i])
}
```

.question[
What is our estimate $\widehat{\pi}_0$?
]
]

---

### So far

.large[
* Our R code suggests that $\widehat{\pi}_i$ maximizes the likelihood
* BUT, we haven't considered all possible values of $\widehat{\pi}_i$
* We could consider more values, but we can't compute a likelihood for every possible $\widehat{\pi}_i$, even in R
* Luckily, we don't have to
]

---

### Maximum likelihood estimation with calculus

.large[
**Step 1:** Write down the likelihood

* Let $\widehat{\pi}_i$ be the estimate of $\pi_i$
* Let $n$ be the number of observations (in coin flipping example, $n = 5$)
* Let $k$ be the number of times $Y_i = 1$ in the data (in coin flipping example, $k = 1$)

.center[
$L(\widehat{\pi}_i) = \widehat{\pi}_i^k(1 - \widehat{\pi}_i)^{n-k}$
]

]

---

### Maximum likelihood estimation with calculus

.large[
**Step 1:** Write down the likelihood

.center[
$L(\widehat{\pi}_i) = \widehat{\pi}_i^k(1 - \widehat{\pi}_i)^{n-k}$
]

**Step 2:** Take the log

.center[
$\log L(\widehat{\pi}_i) = k \log (\widehat{\pi}_i) + (n - k) \log (1 - \widehat{\pi}_i)$
]

* An advantage of taking the log is that it turns multiplication into addition, and exponents into multiplication
* This makes maximization easier
* Maximizing the log likelihood is the same as maximizing the likelihood
]

---

### Maximum likelihood estimation with calculus

.large[
**Step 2:** log likelihood

.center[
$\log L(\widehat{\pi}_i) = k \log (\widehat{\pi}_i) + (n - k) \log (1 - \widehat{\pi}_i)$
]
]

.large[
* We want to find the value of $\widehat{\pi}_i$ that maximizes this function

.question[
How do we find where maxima/minima occur for a function?
]
]

--

.large[
*Take the first derivative and set equal to 0!*
]

---

### Maximum likelihood estimation with calculus

.large[
Want to differentiate
.center[
$\log L(\widehat{\pi}_i) = k \log (\widehat{\pi}_i) + (n - k) \log (1 - \widehat{\pi}_i)$
]
]

.large[
Remember some rules for differentiation:

* $\dfrac{d}{dx} \log x = \dfrac{1}{x}$
* $\dfrac{d}{dx} c f(x) = c \dfrac{d}{dx} f(x)$ for constant $c$
* $\dfrac{d}{dx} (f(x) + g(x)) = \dfrac{d}{dx} f(x) + \dfrac{d}{dx} g(x)$
]

---

### Maximum likelihood estimation with calculus

.large[
**Step 3:** take the first derivative, and set = 0
.center[
$\log L(\widehat{\pi}_i) = k \log (\widehat{\pi}_i) + (n - k) \log (1 - \widehat{\pi}_i)$
]

\begin{align}
\dfrac{d}{d \widehat{\pi}_i} \log L(\widehat{\pi}_i) &= \dfrac{k}{\widehat{\pi}_i} - \dfrac{n - k}{1 - \widehat{\pi}_i} \\
&= 0
\end{align}
]

---

### Maximum likelihood estimation with calculus

.large[
.center[
$\log L(\widehat{\pi}_i) = k \log (\widehat{\pi}_i) + (n - k) \log (1 - \widehat{\pi}_i)$
]

\begin{align}
\dfrac{d}{d \widehat{\pi}_i} \log L(\widehat{\pi}_i) &= \dfrac{k}{\widehat{\pi}_i} - \dfrac{n - k}{1 - \widehat{\pi}_i} \\
&= 0
\end{align}

.center[
$\dfrac{k}{\widehat{\pi}_i} = \dfrac{n - k}{1 - \widehat{\pi}_i}$

$k(1 - \widehat{\pi}_i) = (n-k)\widehat{\pi}_i$

$k - k \widehat{\pi}_i = n \widehat{\pi}_i - k \widehat{\pi}_i$

$\widehat{\pi}_i = \dfrac{k}{n}$
]
]

--

.large[
So our maximum likelihood estimate is $\widehat{\pi}_i = \dfrac{k}{n}$
]

---

### Maximum likelihood estimation with calculus

.large[
So our maximum likelihood estimate is $\widehat{\pi}_i = \dfrac{k}{n}$, the sample proportion

* Our data: T, T, T, T, H
* This implies that $\widehat{\pi}_i = \dfrac{1}{5} = 0.2$
* This matches what we saw in R
]

---

### Class activity, Part II

.large[
[https://sta279-s22.github.io/class_activities/ca_lecture_6.html](https://sta279-s22.github.io/class_activities/ca_lecture_6.html)
]

---

### Class activity, Part II

.large[
.center[
$\log L(\widehat{\pi}_0) = 3 \log(2) + 3 \log(\widehat{\pi}_0) + 2 \log( \widehat{\pi}_0) + \log(1 - 3 \widehat{\pi}_0)$

$\dfrac{d}{d \widehat{\pi}_0} \log L(\widehat{\pi}_0) = \ ?$
]
]

---

### Class activity, Part II

.large[
.center[
$\log L(\widehat{\pi}_0) = 3 \log(2) + 3 \log(\widehat{\pi}_0) + 2 \log( \widehat{\pi}_0) + \log(1 - 3 \widehat{\pi}_0)$

$\dfrac{d}{d \widehat{\pi}_0} \log L(\widehat{\pi}_0) = \dfrac{3}{\widehat{\pi}_0} + \dfrac{2}{\widehat{\pi}_0} - \dfrac{3}{1 - 3 \widehat{\pi}_0} = 0$
]
]

--

.large[
.center[
$\dfrac{5}{\widehat{\pi}_0} = \dfrac{3}{1 - 3\widehat{\pi}_0}$

$\dfrac{1 - 3\widehat{\pi}_0}{\widehat{\pi}_0} = \dfrac{3}{5}$

$\dfrac{1}{\widehat{\pi}_0} = \dfrac{18}{5}$

$\widehat{\pi}_0 = \dfrac{5}{18} = 0.278$
]
]

---

### Next time

.large[
* Maximum likelihood estimation for logistic regression
* Fitting logistic regression in R
]
